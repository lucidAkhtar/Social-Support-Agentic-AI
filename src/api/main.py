"""
FastAPI Backend for Social Support System - Production Grade
Multi-agent system with comprehensive database testing endpoints

Features:
- Application processing endpoints (upload, process, status, results)
- Database testing endpoints (SQLite, TinyDB, ChromaDB, NetworkX)
- RAG chatbot with semantic search
- What-if simulation engine
- System statistics and monitoring

Architecture:
- 4 Databases: SQLite, TinyDB, ChromaDB, NetworkX
- 6 Agents: Extraction, Validation, Eligibility, Recommendation, Explanation, RAG Chatbot
- Orchestrator: Coordinates agent execution
- UnifiedDatabaseManager: Multi-level caching (L1/L2)
"""

from fastapi import FastAPI, UploadFile, File, HTTPException, Form, Query, Path as PathParam
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import JSONResponse
from typing import List, Optional, Dict, Any
from pydantic import BaseModel, Field
import asyncio
import logging
from pathlib import Path
import shutil
import uuid
import json
from datetime import datetime
import os

# Langfuse for production-grade observability
from langfuse import Langfuse

from src.core.langgraph_orchestrator import LangGraphOrchestrator
from src.core.langgraph_state import ApplicationGraphState
from src.core.types import ApplicationState, ProcessingStage
from src.databases import UnifiedDatabaseManager
from src.databases.prod_sqlite_manager import SQLiteManager
from src.databases.tinydb_manager import TinyDBManager
from src.databases.chroma_manager import ChromaDBManager
from src.databases.networkx_manager import NetworkXManager
from src.agents.extraction_agent import DataExtractionAgent
from src.agents.validation_agent import DataValidationAgent
from src.agents.eligibility_agent import EligibilityAgent
from src.agents.recommendation_agent import RecommendationAgent
from src.agents.explanation_agent import ExplanationAgent
from src.agents.rag_chatbot_agent import RAGChatbotAgent

# Configure logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# Import governance and conversation services
from src.services.governance import get_audit_logger, get_structured_logger
from src.services.conversation_manager import get_conversation_manager

# Initialize services
audit_logger = get_audit_logger()
structured_logger = get_structured_logger("social_support_api")
conversation_manager = get_conversation_manager()

# Initialize Langfuse for FastAPI endpoint tracing
langfuse_client = Langfuse(
    public_key=os.getenv("LANGFUSE_PUBLIC_KEY", "local-dev-key"),
    secret_key=os.getenv("LANGFUSE_SECRET_KEY", "local-dev-secret"),
    host=os.getenv("LANGFUSE_HOST", "http://localhost:3000"),
    enabled=True
)
logger.info("Langfuse initialized for FastAPI observability")

# Initialize FastAPI with comprehensive metadata
app = FastAPI(
    title="Social Support System API",
    description="""
# Multi-Agent Social Support Eligibility Assessment System

## Core Features

### Application Processing
- **Document Upload & OCR** - Automated document processing with text extraction
- **6-Agent Pipeline** - Extraction, Validation, Eligibility, Recommendation, Explanation, RAG Chatbot
- **Real-time Status** - Track application progress through all stages
- **Support Calculation** - Automated financial support amount determination

### Intelligent Systems
- **RAG Chatbot** - Ask questions about your application using natural language
- **What-If Simulator** - Explore how changes affect eligibility and support
- **Smart Recommendations** - AI-powered program suggestions based on profile
- **Similar Cases** - Find applications with similar characteristics

### Database Architecture
- **SQLite** - Structured application data with FTS5 full-text search
- **TinyDB** - Session management and caching with TTL
- **ChromaDB** - Vector embeddings for semantic search (120 documents indexed)
- **NetworkX** - Graph relationships (200 nodes, 160 edges)

### Testing Endpoints
Comprehensive testing endpoints for all 4 databases with example data in Swagger UI.

---
"""

)

# CORS middleware for Streamlit and web clients
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# Production-grade audit middleware
from fastapi import Request
import time

@app.middleware("http")
async def audit_middleware(request: Request, call_next):
    """
    Automatic API access logging and performance monitoring
    FAANG Standards: Track every request for governance
    """
    start_time = time.time()
    
    # Extract application_id from path if present
    application_id = None
    if "/applications/" in request.url.path:
        path_parts = request.url.path.split("/applications/")
        if len(path_parts) > 1:
            application_id = path_parts[1].split("/")[0]
    
    try:
        response = await call_next(request)
        response_time_ms = int((time.time() - start_time) * 1000)
        
        # Log API access
        audit_logger.log_api_access(
            method=request.method,
            endpoint=request.url.path,
            application_id=application_id,
            status_code=response.status_code,
            response_time_ms=response_time_ms,
            ip_address=request.client.host,
            user_agent=request.headers.get("user-agent")
        )
        
        # Log performance metric
        audit_logger.log_metric(
            f"api.{request.method}.response_time",
            response_time_ms,
            "ms",
            {"method": request.method, "endpoint": request.url.path, "status": response.status_code}
        )
        
        # Add performance headers
        response.headers["X-Response-Time"] = f"{response_time_ms}ms"
        
        return response
        
    except Exception as e:
        response_time_ms = int((time.time() - start_time) * 1000)
        
        # Log failed request
        audit_logger.log_audit_event(
            event_type="api_error",
            action=request.method,
            resource=request.url.path,
            application_id=application_id,
            status="error",
            error_message=str(e),
            ip_address=request.client.host,
            user_agent=request.headers.get("user-agent")
        )
        
        raise

# Initialize orchestrator and unified database manager
orchestrator = LangGraphOrchestrator()
unified_db = UnifiedDatabaseManager()

# Initialize individual database managers for testing endpoints
sqlite_db = SQLiteManager("data/databases/applications.db")
tinydb_cache = TinyDBManager("data/databases/cache.json")
chroma_db = ChromaDBManager("data/databases/chromadb")  # Updated to use main chromadb with 828 documents
networkx_db = NetworkXManager()

# Load NetworkX graph from file
try:
    import networkx as nx
    graph_path = Path("application_graph.graphml")
    if graph_path.exists():
        networkx_db.graph = nx.read_graphml(str(graph_path))
        logger.info(f"NetworkX loaded: {networkx_db.graph.number_of_nodes()} nodes, {networkx_db.graph.number_of_edges()} edges")
    else:
        # Fresh system - graph will be created on first application
        logger.debug("NetworkX graph file not found - will be created on first use")
except Exception as e:
    logger.error(f"NetworkX load failed: {e}")


# Initialize and register all agents with comprehensive configuration
extraction_agent = DataExtractionAgent()
validation_agent = DataValidationAgent()
eligibility_agent = EligibilityAgent()
recommendation_agent = RecommendationAgent()
explanation_agent = ExplanationAgent()
rag_chatbot_agent = RAGChatbotAgent({
    'db_path': 'data/databases/applications.db',
    'ollama_url': 'http://localhost:11434',
    'ollama_model': 'mistral:latest'
})

orchestrator.register_agents(
    extraction_agent=extraction_agent,
    validation_agent=validation_agent,
    eligibility_agent=eligibility_agent,
    recommendation_agent=recommendation_agent,
    explanation_agent=explanation_agent,
    rag_chatbot_agent=rag_chatbot_agent
)
logger.info("All 6 agents initialized and registered (including RAG chatbot)")

# In-memory state storage (production: use Redis/Memcached with TTL)
active_applications: Dict[str, ApplicationState] = {}


# ============================================================================
# PYDANTIC MODELS - Input Validation & Documentation
# ============================================================================

class ApplicationInput(BaseModel):
    """Input for SQLite application insertion (DATABASE TESTING)"""
    app_id: str = Field(..., example="APP-000001", description="Unique application ID")
    applicant_name: str = Field(..., example="Ahmed Hassan", description="Full name of applicant")
    emirates_id: str = Field(..., example="784-1990-1234567-1", description="Emirates ID")
    submission_date: str = Field(..., example="2024-12-01", description="Submission date")
    status: str = Field(default="PENDING", example="PENDING", description="Application status")
    monthly_income: float = Field(..., example=5200.0, ge=0, description="Monthly income in AED")
    monthly_expenses: float = Field(..., example=3800.0, ge=0, description="Monthly expenses in AED")
    family_size: int = Field(..., example=4, ge=1, description="Family size (number of dependents)")
    employment_status: str = Field(..., example="Government Employee", description="Employment status")
    total_assets: float = Field(..., example=85000.0, ge=0, description="Total assets value")
    total_liabilities: float = Field(..., example=42000.0, ge=0, description="Total liabilities")
    credit_score: int = Field(..., example=680, ge=300, le=850, description="Credit score (300-850)")
    policy_score: Optional[float] = Field(None, example=72.5, description="Calculated policy score")
    eligibility: Optional[str] = Field(None, example="APPROVED", description="Eligibility decision")
    support_amount: Optional[float] = Field(None, example=5000.0, description="Support amount in AED")


class SessionInput(BaseModel):
    """Input for TinyDB session management (DATABASE TESTING)"""
    session_id: str = Field(..., example="user_12345", description="Unique session identifier")
    data: Dict[str, Any] = Field(
        ..., 
        example={"user_id": "user_12345", "language": "en", "current_app_id": "APP-000001"},
        description="Session data to store"
    )


class RAGQueryInput(BaseModel):
    """Input for ChromaDB semantic search (DATABASE TESTING)"""
    query: str = Field(..., example="low income large family unemployed", description="Natural language query")
    n_results: int = Field(default=5, example=5, ge=1, le=20, description="Number of results to return")


class GraphQueryInput(BaseModel):
    """Input for NetworkX graph queries (DATABASE TESTING)"""
    node_type: Optional[str] = Field(None, example="Application", description="Filter by node type")
    attribute_filter: Optional[Dict[str, Any]] = Field(
        None, 
        example={"eligibility": "APPROVED"},
        description="Filter by node attributes"
    )


class ChatQuery(BaseModel):
    """Input for RAG chatbot queries"""
    application_id: str = Field(..., example="APP-000001", description="Application ID to query about")
    query: str = Field(
        ..., 
        example="Why was my application approved?",
        description="Natural language question about the application"
    )
    query_type: Optional[str] = Field(
        default="explanation",
        example="explanation",
        description="Query type: explanation, simulation, details, similar_cases"
    )


class SimulationQuery(BaseModel):
    """Input for what-if simulation"""
    application_id: str = Field(..., example="APP-000001", description="Application ID to simulate")
    changes: Dict[str, Any] = Field(
        ...,
        example={"monthly_income": 8000, "employment_status": "Government Employee"},
        description="Changes to simulate (e.g., income increase, employment change)"
    )


class ApplicationResponse(BaseModel):
    """Response for application creation"""
    application_id: str
    status: str
    current_stage: str
    message: str


class ProcessingStatusResponse(BaseModel):
    """Response for processing status"""
    application_id: str
    current_stage: str
    message: str
    data: Optional[Dict[str, Any]] = None


# ============================================================================
# CORE APPLICATION ENDPOINTS
# ============================================================================

@app.get("/", tags=["System"])
async def root():
    """
    System health check and API information.
    
    **Returns:** API metadata, version, and health status
    """
    return {
        "status": "healthy",
        "service": "Social Support System API - Production Grade",
        "version": "2.0.0",
        "api_version": "v1",
        "note": "All endpoints are versioned at /api/v1/* for future compatibility",
        "databases": {
            "sqlite": "operational",
            "tinydb": "operational",
            "chromadb": "operational",
            "networkx": "operational"
        },
        "agents": {
            "extraction": "ready",
            "validation": "ready",
            "eligibility": "ready",
            "recommendation": "ready",
            "explanation": "ready",
            "rag_chatbot": "ready"
        }
    }


# API Version 1 Routes
@app.post("/api/v1/applications/create", response_model=ApplicationResponse, tags=["Applications v1"])
@app.post("/api/applications/create", response_model=ApplicationResponse, tags=["Applications v1"], include_in_schema=False)
async def create_application(applicant_name: str = Form(..., example="Ahmed Hassan Al Mazrouei")):
    """
    Create a new application for processing.
    
    **TEST DATA:**
    ```
    applicant_name: "Ahmed Hassan Al Mazrouei"
    applicant_name: "Fatima Mohammed Al Dhaheri"
    applicant_name: "Khalid Abdullah Al Nuaimi"
    ```
    
    **Process:**
    1. Generates unique application ID
    2. Creates database record
    3. Initializes application state
    4. Returns application ID for document upload
    
    **Next Step:** Upload documents using `/api/applications/{application_id}/upload`
    """
    try:
        application_id = f"APP_{uuid.uuid4().hex[:8].upper()}"
        
        # Create in SQLite database
        app_data = {
            "app_id": application_id,
            "applicant_name": applicant_name.strip(),
            "submission_date": datetime.now().strftime("%Y-%m-%d"),
            "status": "PENDING",
            "monthly_income": 0.0,
            "monthly_expenses": 0.0,
            "family_size": 1,
            "employment_status": "Unknown",
            "total_assets": 0.0,
            "total_liabilities": 0.0,
            "credit_score": 0,
            "emirates_id": ""
        }
        sqlite_db.insert_application(app_data)
        
        # Create state
        state = ApplicationState(
            application_id=application_id,
            applicant_name=applicant_name.strip(),
            stage=ProcessingStage.PENDING
        )
        active_applications[application_id] = state
        
        logger.info(f"Created application {application_id} for {applicant_name}")
        
        return ApplicationResponse(
            application_id=application_id,
            status="created",
            current_stage=state.stage.value,
            message=f"Application created successfully for {applicant_name}"
        )
    
    except Exception as e:
        logger.error(f"Error creating application: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@app.post("/api/applications/{application_id}/upload", tags=["Applications"])
async def upload_documents(
    application_id: str = PathParam(..., example="APP_12AB34CD", description="Application ID from create endpoint"),
    documents: List[UploadFile] = File(..., description="PDF/Image documents (Emirates ID, bank statements, etc.)")
):
    """
    Upload documents for OCR processing.
    
    **TEST DATA:**
    ```
    application_id: "APP-000001"  (Use ID from create endpoint)
    documents: 
      - emirates_id.pdf (Emirates ID card scan)
      - bank_statement.pdf (3-month bank statement)
      - utility_bill.pdf (Recent utility bill)
      - salary_certificate.pdf (Employment certificate)
    ```
    
    **Supported Document Types:**
    - Emirates ID (emirates_id, id)
    - Resume/CV (resume, cv)
    - Bank Statement (bank, statement)
    - Utility Bill (utility, bill)
    - Assets/Liabilities (asset, liability)
    - Other documents
    
    **Process:**
    1. Validates application exists
    2. Saves uploaded files to disk
    3. Classifies document types automatically
    4. Stores document metadata in database
    5. Ready for processing pipeline
    
    **Next Step:** Process application using `/api/applications/{application_id}/process`
    """
    try:
        if application_id not in active_applications:
            raise HTTPException(status_code=404, detail="Application not found")
        
        state = active_applications[application_id]
        upload_dir = Path(f"data/uploads/{application_id}")
        upload_dir.mkdir(parents=True, exist_ok=True)
        
        uploaded_files = []
        
        for file in documents:
            # Save file
            file_path = upload_dir / file.filename
            with open(file_path, "wb") as buffer:
                shutil.copyfileobj(file.file, buffer)
            
            # Determine document type from filename
            filename_lower = file.filename.lower()
            if "emirates" in filename_lower or ("id" in filename_lower and "credit" not in filename_lower):
                doc_type = "emirates_id"
            elif "resume" in filename_lower or "cv" in filename_lower:
                doc_type = "resume"
            elif "bank" in filename_lower or "statement" in filename_lower:
                doc_type = "bank_statement"
            elif "utility" in filename_lower or "bill" in filename_lower:
                doc_type = "utility_bill"
            elif "asset" in filename_lower or "liab" in filename_lower:
                doc_type = "assets_liabilities"
            elif "credit" in filename_lower:
                doc_type = "credit_report"
            elif "employment" in filename_lower or "letter" in filename_lower:
                doc_type = "employment_letter"
            else:
                doc_type = "other"
            
            # Add to state
            from src.core.types import Document
            doc = Document(
                document_id=f"DOC_{uuid.uuid4().hex[:8].upper()}",
                document_type=doc_type,
                filename=file.filename,
                file_path=str(file_path)
            )
            state.documents.append(doc)
            
            # Save to database
            # Note: Document metadata saved in SQLite via unified_db
            # unified_db.sqlite.add_document(application_id, doc.document_id, doc_type, str(file_path))
            
            uploaded_files.append({
                "filename": file.filename,
                "document_type": doc_type,
                "document_id": doc.document_id
            })
        
        # Log action to TinyDB
        # unified_db.tinydb.store_session(f"{application_id}_upload", 
        #     {"count": len(documents), "files": uploaded_files})
        
        logger.info(f"Uploaded {len(documents)} documents for {application_id}")
        
        return {
            "application_id": application_id,
            "uploaded_count": len(documents),
            "documents": uploaded_files,
            "message": f"Successfully uploaded {len(documents)} document(s)"
        }
    
    except Exception as e:
        logger.error(f"Error uploading documents: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@app.post("/api/applications/{application_id}/process", response_model=ProcessingStatusResponse, tags=["Applications"])
async def process_application(
    application_id: str = PathParam(..., example="APP-000001", description="Application ID to process")
):
    """
    Process application through all 6 agents with full Langfuse observability.
    
    **TEST DATA:**
    ```
    application_id: "APP-000001"  (Must have uploaded documents)
    application_id: "APP-000010"  (Another test application)
    ```
    
    **Processing Pipeline:**
    1. **Extraction Agent** → Extracts data from uploaded documents using OCR
    2. **Validation Agent** → Validates extracted data for completeness and accuracy
    3. **Eligibility Agent** → Determines eligibility based on policy rules + ML prediction
    4. **Recommendation Agent** → Recommends support amount and programs
    5. **Explanation Agent** → Generates human-readable explanation
    6. **RAG Chatbot** → Indexes application data for Q&A
    
    **Expected Stages:**
    - PENDING → EXTRACTING → VALIDATING → DECIDING → COMPLETED
    
    **Processing Time:** 30-60 seconds for full pipeline
    
    **Langfuse Tracing:** Full trace exported to data/observability/langfuse_trace_{app_id}.json
    
    **Next Step:** Check results using `/api/applications/{application_id}/results`
    """
    # Start Langfuse trace for entire HTTP request
    request_trace = langfuse_client.trace(
        name="fastapi_process_application",
        id=f"api_trace_{application_id}_{int(datetime.now().timestamp())}",
        metadata={
            "endpoint": "/api/applications/{id}/process",
            "application_id": application_id,
            "timestamp": datetime.now().isoformat()
        }
    )
    
    request_span = request_trace.span(name="process_application_endpoint")
    
    try:
        if application_id not in active_applications:
            request_span.end(output={"success": False, "error": "Application not found"}, level="ERROR")
            langfuse_client.flush()
            raise HTTPException(status_code=404, detail="Application not found")
        
        state = active_applications[application_id]
        
        # Register application with orchestrator if not already registered
        if application_id not in orchestrator.applications:
            orchestrator.applications[application_id] = state
        
        # Process through orchestrator (this will create its own nested Langfuse trace)
        langgraph_span = request_trace.span(name="langgraph_orchestrator_execution")
        
        # Convert documents from ApplicationState to list format required by LangGraphOrchestrator
        documents_list = []
        if hasattr(state, 'documents') and state.documents:
            documents_list = [
                {
                    "document_id": doc.document_id,
                    "document_type": doc.document_type,
                    "file_path": doc.file_path,
                    "filename": doc.filename
                }
                for doc in state.documents
            ]
        
        try:
            final_state = await orchestrator.process_application(
                application_id=application_id,
                applicant_name=state.applicant_name,
                documents=documents_list
            )
            
            # DEBUG: Log what name we're using
            logger.info(f"[{application_id}] Processing with applicant_name: '{state.applicant_name}'")
            
            if final_state is None:
                raise ValueError("Orchestrator returned None - workflow failed")
            
            langgraph_span.end(output={
                "success": True,
                "final_stage": final_state.get("stage") if final_state else "UNKNOWN",
                "is_eligible": final_state.get("eligibility_result").is_eligible if (final_state and final_state.get("eligibility_result")) else False
            })
            
        except Exception as workflow_error:
            langgraph_span.end(output={"success": False, "error": str(workflow_error)}, level="ERROR")
            raise
        
        # Save all results to database
        db_span = request_trace.span(name="database_persistence")
        
        # Check if we have valid extracted data before accessing attributes
        if final_state and final_state.get("extracted_data"):
            extracted_data = final_state["extracted_data"]
            # CRITICAL FIX: Add None checks for nested dictionaries
            credit_data = extracted_data.credit_data if extracted_data.credit_data else {}
            employment_data = extracted_data.employment_data if extracted_data.employment_data else {}
            income_data = extracted_data.income_data if extracted_data.income_data else {}
            applicant_info = extracted_data.applicant_info if extracted_data.applicant_info else {}
            family_info = extracted_data.family_info if extracted_data.family_info else {}
            assets_liabilities = extracted_data.assets_liabilities if extracted_data.assets_liabilities else {}
            
            # DEBUG: Log extracted data before saving
            logger.info(f"[{application_id}] DEBUG - credit_data keys: {list(credit_data.keys())}")
            logger.info(f"[{application_id}] DEBUG - credit_score value: {credit_data.get('credit_score')}")
            logger.info(f"[{application_id}] DEBUG - employment_data keys: {list(employment_data.keys())}")
            logger.info(f"[{application_id}] DEBUG - company_name value: {employment_data.get('company_name')}")
            
            # Prepare application data for database
            # CRITICAL: Ensure all NOT NULL fields have valid defaults
            app_data = {
                "app_id": application_id,
                "applicant_name": applicant_info.get("full_name", "Unknown"),
                "emirates_id": applicant_info.get("id_number", ""),
                "submission_date": datetime.now().strftime("%Y-%m-%d"),
                "status": final_state["stage"],
                "monthly_income": float(income_data.get("monthly_income") or 0),
                "monthly_expenses": float(income_data.get("monthly_expenses") or 0),
                "family_size": int(family_info.get("family_size") or 1),
                "employment_status": employment_data.get("employment_status") or "Unknown",
                "total_assets": float(assets_liabilities.get("total_assets") or 0),
                "total_liabilities": float(assets_liabilities.get("total_liabilities") or 0),
                "credit_score": int(credit_data.get("credit_score") or 0),  # FIXED: Convert to int, default 0
                "policy_score": final_state["eligibility_result"].eligibility_score if final_state.get("eligibility_result") else None,
                "ml_prediction": str(final_state["eligibility_result"].ml_prediction) if final_state.get("eligibility_result") else None,
                "ml_confidence": None,
                "eligibility": "ELIGIBLE" if (final_state.get("eligibility_result") and final_state["eligibility_result"].is_eligible) else "NOT_ELIGIBLE",
                "support_amount": float(final_state["recommendation"].financial_support_amount) if final_state.get("recommendation") else 0.0,
                # New fields from enhanced extraction (nullable fields can be None)
                "company_name": employment_data.get("company_name"),
                "current_position": employment_data.get("current_position"),
                "join_date": employment_data.get("join_date"),
                "credit_rating": credit_data.get("credit_rating"),
                "credit_accounts": json.dumps(credit_data.get("credit_accounts", [])),
                "payment_ratio": float(credit_data.get("payment_history", {}).get("payment_ratio")) if (credit_data.get("payment_history") and credit_data.get("payment_history", {}).get("payment_ratio") is not None) else None,
                "total_outstanding": float(credit_data.get("total_outstanding")) if credit_data.get("total_outstanding") is not None else None,
                "work_experience_years": int(employment_data.get("experience_years")) if employment_data.get("experience_years") is not None else None,
                "education_level": employment_data.get("education_level")
            }
            # DEBUG: Log prepared app_data before DB insert
            logger.info(f"[{application_id}] DEBUG - app_data credit_score: {app_data.get('credit_score')}")
            logger.info(f"[{application_id}] DEBUG - app_data company_name: {app_data.get('company_name')}")
            logger.info(f"[{application_id}] DEBUG - app_data credit_rating: {app_data.get('credit_rating')}")
            # Save to SQLite
            unified_db.sqlite.insert_application(app_data)
            logger.info(f"Saved application data to SQLite for {application_id}")
        
        if final_state.get("validation_report"):
            validation_report = final_state["validation_report"]
            validation_data = {
                "is_valid": validation_report.is_valid,
                "completeness_score": validation_report.data_completeness_score,
                "confidence_score": validation_report.confidence_score,
                "issues": [{"field": i.field, "severity": i.severity, "message": i.message} 
                          for i in validation_report.issues]
            }
            # Store validation metrics in analytics
            unified_db.sqlite.update_analytics(
                f"{application_id}_validation",
                validation_report.data_completeness_score,
                validation_data
            )
            logger.info(f"Saved validation data for {application_id}")
        
        if final_state.get("eligibility_result") and final_state.get("recommendation"):
            eligibility = final_state["eligibility_result"]
            recommendation = final_state["recommendation"]
            decision_data = {
                "decision_id": f"DEC_{application_id}",
                "app_id": application_id,
                "decision": recommendation.decision.value,
                "decision_date": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
                "decided_by": "SYSTEM",
                "policy_score": eligibility.eligibility_score,
                "ml_score": eligibility.ml_prediction.get("probability") if (eligibility.ml_prediction and isinstance(eligibility.ml_prediction, dict)) else None,
                "priority": "high" if eligibility.is_eligible else "low",
                "reasoning": json.dumps(eligibility.reasoning),
                "support_type": recommendation.financial_support_type,
                "support_amount": recommendation.financial_support_amount,
                "duration_months": None,  # Duration not in Recommendation dataclass - set NULL
                "conditions": json.dumps([prog.get("program_name") if isinstance(prog, dict) else prog for prog in recommendation.economic_enablement_programs]) if recommendation.economic_enablement_programs else None
            }
            # Save decision to SQLite
            unified_db.sqlite.insert_decision(decision_data)
            logger.info(f"Saved decision data to SQLite for {application_id}")
        
        if final_state.get("recommendation"):
            recommendation = final_state["recommendation"]
            recommendation_data = {
                "decision_type": recommendation.decision.value,
                "financial_support_amount": recommendation.financial_support_amount,
                "financial_support_type": recommendation.financial_support_type,
                "programs": recommendation.economic_enablement_programs,
                "reasoning": recommendation.reasoning
            }
            # Store recommendation summary in analytics
            unified_db.sqlite.update_analytics(
                f"{application_id}_recommendation",
                recommendation.financial_support_amount,
                recommendation_data
            )
            logger.info(f"Saved recommendation data for {application_id}")
        
        db_span.end(output={"success": True, "records_saved": "application, validation, decision, recommendation"})
        
        # Update state
        active_applications[application_id] = final_state
        logger.info(f"Application {application_id} processing complete - all data saved to database")
        
        # Prepare response
        progress = 100 if final_state["stage"] == ProcessingStage.COMPLETED else 50
        
        response_data = ProcessingStatusResponse(
            application_id=application_id,
            current_stage=final_state["stage"],
            progress_percentage=progress,
            message="Application processing completed",
            data={
                "extracted_data": final_state.get("extracted_data") is not None,
                "validation_valid": final_state["validation_report"].is_valid if final_state.get("validation_report") else False,
                "eligibility_score": final_state["eligibility_result"].eligibility_score if final_state.get("eligibility_result") else 0,
                "decision": final_state["recommendation"].decision.value if final_state.get("recommendation") else "PENDING"
            }
        )
        
        # End request span with successful response
        request_span.end(output={
            "success": True,
            "stage": final_state["stage"],
            "is_eligible": final_state["eligibility_result"].is_eligible if final_state.get("eligibility_result") else False,
            "support_amount": final_state["recommendation"].financial_support_amount if final_state.get("recommendation") else 0
        })
        
        # Flush Langfuse to ensure trace is written
        langfuse_client.flush()
        
        logger.info(f"Langfuse trace completed for API request: {application_id}")
        
        return response_data
    
    except Exception as e:
        logger.error(f"Error processing application: {e}")
        request_span.end(output={"success": False, "error": str(e)}, level="ERROR")
        langfuse_client.flush()
        raise HTTPException(status_code=500, detail=str(e))


@app.get("/api/applications/{application_id}/status", response_model=ProcessingStatusResponse, tags=["Applications"])
async def get_application_status(
    application_id: str = PathParam(..., example="APP-000001", description="Application ID to check")
):
    """
    Get current processing status of application.
    
    **TEST DATA:**
    ```
    application_id: "APP-000001"  (Check processing stage)
    application_id: "APP-000010"  (Another test application)
    ```
    
    **Possible Stages:**
    - **PENDING** (0%) - Application created, awaiting document upload
    - **EXTRACTING** (20%) - Extraction Agent processing documents with OCR
    - **VALIDATING** (40%) - Validation Agent checking data completeness
    - **CHECKING_ELIGIBILITY** (60%) - Eligibility Agent determining eligibility
    - **GENERATING_RECOMMENDATION** (80%) - Recommendation Agent calculating support
    - **COMPLETED** (100%) - All agents finished, results ready
    - **FAILED** (0%) - Error during processing
    
    **Example Response:**
    ```json
    {
      "application_id": "APP-000001",
      "current_stage": "COMPLETED",
      "progress_percentage": 100,
      "message": "Application is in COMPLETED stage",
      "data": {
        "documents_count": 5,
        "has_extracted_data": true,
        "has_validation": true,
        "has_eligibility": true,
        "has_recommendation": true
      }
    }
    ```
    """
    try:
        if application_id not in active_applications:
            # Try to load from database
            app_data = sqlite_db.get_application(application_id)
            if not app_data:
                raise HTTPException(status_code=404, detail="Application not found")
            
            return ProcessingStatusResponse(
                application_id=application_id,
                current_stage=app_data['current_stage'],
                progress_percentage=0,
                message="Application found in database",
                data=app_data
            )
        
        state = active_applications[application_id]
        
        # Handle both ApplicationState (old) and ApplicationGraphState (new dict from LangGraph)
        if isinstance(state, dict):
            # LangGraph state (dict)
            stage = state.get("stage", "PENDING")
            documents_count = len(state.get("documents", []))
            has_extracted = state.get("extracted_data") is not None
            has_validation = state.get("validation_report") is not None
            has_eligibility = state.get("eligibility_result") is not None
            has_recommendation = state.get("recommendation") is not None
        else:
            # Old ApplicationState object
            stage = state.stage.value if hasattr(state.stage, 'value') else state.stage
            documents_count = len(state.documents)
            has_extracted = state.extracted_data is not None
            has_validation = state.validation_report is not None
            has_eligibility = state.eligibility_result is not None
            has_recommendation = state.recommendation is not None
        
        stage_progress = {
            "PENDING": 0,
            "EXTRACTING": 20,
            "extracting": 20,
            "VALIDATING": 40,
            "validating": 40,
            "CHECKING_ELIGIBILITY": 60,
            "checking_eligibility": 60,
            "GENERATING_RECOMMENDATION": 80,
            "generating_recommendation": 80,
            "COMPLETED": 100,
            "completed": 100,
            "FAILED": 0,
            "failed": 0
        }
        
        return ProcessingStatusResponse(
            application_id=application_id,
            current_stage=stage,
            progress_percentage=stage_progress.get(stage, 0),
            message=f"Application is in {stage} stage",
            data={
                "documents_count": documents_count,
                "has_extracted_data": has_extracted,
                "has_validation": has_validation,
                "has_eligibility": has_eligibility,
                "has_recommendation": has_recommendation
            }
        )
    
    except Exception as e:
        logger.error(f"Error getting status: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@app.get("/api/applications/{application_id}/results", tags=["Applications"])
async def get_application_results(
    application_id: str = PathParam(..., example="APP-000001", description="Application ID to get results for")
):
    """
    Get complete results from all agents.
    
    **TEST DATA:**
    ```
    application_id: "APP-000001"  (Must be COMPLETED)
    application_id: "APP-000010"  (Another completed application)
    ```
    
    **Returns Complete Data from All Agents:**
    
    **1. Extraction Agent:**
    - Applicant info (name, Emirates ID, age, nationality)
    - Income data (monthly income, expenses, net income)
    - Employment data (status, employer, experience)
    - Assets & liabilities (assets, debts, net worth)
    - Credit data (credit score, history)
    - Family info (size, dependents, children)
    
    **2. Validation Agent:**
    - Data validity (is_valid: true/false)
    - Completeness score (0-100%)
    - Confidence score (0-100%)
    - Issues list (missing fields, inconsistencies)
    
    **3. Eligibility Agent:**
    - Eligibility determination (is_eligible: true/false)
    - Eligibility score (0-100)
    - ML prediction (probability 0-1)
    - Policy rules met (list of criteria)
    - Reasoning (detailed explanation)
    
    **4. Recommendation Agent:**
    - Decision (APPROVED, CONDITIONAL, DECLINED)
    - Support amount (AED)
    - Support type (UNCONDITIONAL_CASH, CONDITIONAL_CASH, etc.)
    - Programs recommended (list)
    - Reasoning
    
    **5. Explanation Agent:**
    - Human-readable explanation
    - Key factors (positive/negative)
    - Decision justification
    
    **Example Response Structure:**
    ```json
    {
      "application_id": "APP-000001",
      "current_stage": "COMPLETED",
      "extracted_data": {...},
      "validation": {...},
      "eligibility": {...},
      "recommendation": {...},
      "explanation": {...}
    }
    ```
    """
    try:
        if application_id not in active_applications:
            # Try to load from database if not in memory
            db_data = sqlite_db.get_application(application_id)
            if not db_data:
                raise HTTPException(
                    status_code=404, 
                    detail=f"Application {application_id} not found. Please verify the application ID."
                )
            
            # Return database data only
            return {
                "application_id": application_id,
                "current_stage": "COMPLETED",
                "message": "Application data retrieved from database (not actively processed in this session)",
                "database_data": db_data,
                "note": "For real-time agent processing, create a new application and upload documents."
            }
        
        state = active_applications[application_id]
        
        # Handle both ApplicationState (old) and ApplicationGraphState (new dict from LangGraph)
        if isinstance(state, dict):
            # LangGraph state (dict)
            stage = state.get("stage", "UNKNOWN")
            extracted_data = state.get("extracted_data")
            validation_report = state.get("validation_report")
            eligibility_result = state.get("eligibility_result")
            recommendation = state.get("recommendation")
            explanation = state.get("explanation")
        else:
            # Old ApplicationState object
            stage = state.stage.value if hasattr(state.stage, 'value') else state.stage
            extracted_data = state.extracted_data
            validation_report = state.validation_report
            eligibility_result = state.eligibility_result
            recommendation = state.recommendation
            explanation = state.explanation
        
        # Get complete data from database
        try:
            full_data = sqlite_db.get_application(application_id)
            if not full_data:
                logger.warning(f"Application {application_id} not found in SQLite database")
                full_data = {}
        except Exception as e:
            logger.error(f"Error retrieving application from database: {e}")
            full_data = {}
        
        return {
            "application_id": application_id,
            "current_stage": stage,
            "extracted_data": {
                "applicant_info": extracted_data.applicant_info,
                "income_data": extracted_data.income_data,
                "employment_data": extracted_data.employment_data,
                "assets_liabilities": extracted_data.assets_liabilities,
                "credit_data": extracted_data.credit_data,
                "family_info": extracted_data.family_info,
            } if extracted_data else None,
            "database_stored_fields": {
                "company_name": full_data.get('company_name'),
                "current_position": full_data.get('current_position'),
                "join_date": full_data.get('join_date'),
                "credit_score": full_data.get('credit_score'),
                "credit_rating": full_data.get('credit_rating'),
                "payment_ratio": full_data.get('payment_ratio'),
                "total_outstanding": full_data.get('total_outstanding'),
                "work_experience_years": full_data.get('work_experience_years'),
                "education_level": full_data.get('education_level')
            } if full_data else None,
            "validation": {
                "is_valid": validation_report.is_valid,
                "completeness_score": validation_report.data_completeness_score,
                "confidence_score": validation_report.confidence_score,
                "issues": [{"field": i.field, "severity": i.severity, "message": i.message}
                          for i in validation_report.issues]
            } if validation_report else None,
            "eligibility": {
                "is_eligible": eligibility_result.is_eligible,
                "eligibility_score": eligibility_result.eligibility_score,
                "ml_prediction": eligibility_result.ml_prediction,
                "policy_rules_met": eligibility_result.policy_rules_met,
                "reasoning": eligibility_result.reasoning
            } if eligibility_result else None,
            "recommendation": {
                "decision": recommendation.decision.value,
                "support_amount": recommendation.financial_support_amount,
                "support_type": recommendation.financial_support_type,
                "programs": recommendation.economic_enablement_programs,
                "reasoning": recommendation.reasoning,
                "key_factors": recommendation.key_factors
            } if recommendation else None,
            "explanation": {
                "summary": explanation.summary,
                "detailed_reasoning": explanation.detailed_reasoning,
                "factors_analysis": explanation.factors_analysis
            } if explanation else None,
            "database_data": full_data
        }
    
    except Exception as e:
        logger.error(f"Error getting results: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@app.post("/api/applications/{application_id}/chat", tags=["Applications"])
async def chat_with_agent(chat_query: ChatQuery):
    """
    Chat with RAG-powered explanation agent with Langfuse tracing.
    
    **TEST DATA (Copy for Swagger UI - Request Body):**
    
    **Explanation Queries:**
    ```json
    {
      "application_id": "APP-000001",
      "query": "Why was my application approved?",
      "query_type": "explanation"
    }
    ```
    
    ```json
    {
      "application_id": "APP-000001",
      "query": "What factors influenced the decision?",
      "query_type": "explanation"
    }
    ```
    
    **Simulation Queries:**
    ```json
    {
      "application_id": "APP-000001",
      "query": "What if my income increases to 8000 AED?",
      "query_type": "simulation"
    }
    ```
    
    ```json
    {
      "application_id": "APP-000001",
      "query": "How would getting a government job affect my eligibility?",
      "query_type": "simulation"
    }
    ```
    
    **Similar Cases Queries:**
    ```json
    {
      "application_id": "APP-000001",
      "query": "Show me similar cases with the same profile",
      "query_type": "similar_cases"
    }
    ```
    
    **Details Queries:**
    ```json
    {
      "application_id": "APP-000001",
      "query": "What is my policy score?",
      "query_type": "details"
    }
    ```
    
    **How It Works:**
    1. Query passed to RAG Chatbot Agent
    2. Retrieves relevant context from ChromaDB (semantic search)
    3. Uses GPT-4 to generate contextual response
    4. Caches in TinyDB for fast repeated queries
    5. Saves conversation history
    
    **Langfuse Tracing:** Full chat trace exported
    
    **Response Time:** 1-3 seconds (with ChromaDB + GPT-4)
    """
    # Start Langfuse trace for chat request
    chat_trace = langfuse_client.trace(
        name="fastapi_chat_request",
        id=f"chat_trace_{chat_query.application_id}_{int(datetime.now().timestamp())}",
        metadata={
            "endpoint": "/api/applications/{id}/chat",
            "application_id": chat_query.application_id,
            "query_type": chat_query.query_type,
            "timestamp": datetime.now().isoformat()
        }
    )
    
    chat_span = chat_trace.span(name="rag_chatbot_query")
    
    try:
        application_id = chat_query.application_id
        
        # PRODUCTION FIX: Load application data from database if not in active sessions
        if application_id not in active_applications:
            # Retrieve full application data from database
            db_data = sqlite_db.get_application(application_id)
            
            if not db_data:
                raise HTTPException(
                    status_code=404, 
                    detail=f"Application {application_id} not found in database. Please verify the application ID."
                )
            
            # Create ApplicationState from database data for chatbot context
            state = ApplicationState(
                application_id=application_id,
                applicant_name=db_data.get('applicant_name', 'Unknown')
            )
            
            # CRITICAL FIX: Store extracted data as dict (RAG engine expects dict, not ExtractedData object)
            # This is passed to chatbot which converts to proper format internally
            extracted_data_dict = {
                # Basic info
                'monthly_income': db_data.get('monthly_income', 0),
                'monthly_expenses': db_data.get('monthly_expenses', 0),
                'family_size': db_data.get('family_size', 1),
                'employment_status': db_data.get('employment_status', 'Unknown'),
                'total_assets': db_data.get('total_assets', 0),
                'total_liabilities': db_data.get('total_liabilities', 0),
                'credit_score': db_data.get('credit_score', 0),
                'net_worth': db_data.get('net_worth', 0),
                'emirates_id': db_data.get('emirates_id', ''),
                'applicant_name': db_data.get('applicant_name', 'Unknown'),
                # NEW FIELDS - Employment details
                'company_name': db_data.get('company_name'),
                'current_position': db_data.get('current_position'),
                'join_date': db_data.get('join_date'),
                'work_experience_years': db_data.get('work_experience_years'),
                'education_level': db_data.get('education_level'),
                # NEW FIELDS - Credit details
                'credit_rating': db_data.get('credit_rating'),
                'payment_ratio': db_data.get('payment_ratio'),
                'total_outstanding': db_data.get('total_outstanding'),
                'credit_accounts': db_data.get('credit_accounts')
            }
            
            # Store as simple attribute (not ExtractedData object)
            state.extracted_data = extracted_data_dict
            
            # Populate eligibility result similarly
            eligibility_dict = None
            if db_data.get('decision'):
                eligibility_dict = {
                    'eligible': db_data.get('decision') in ['APPROVED', 'CONDITIONAL'],
                    'policy_score': db_data.get('policy_score', 0),
                    'ml_score': db_data.get('ml_score', 0),
                    'decision': db_data.get('decision', 'PENDING'),
                    'support_amount': db_data.get('support_amount', 0),
                    'support_type': db_data.get('support_type', ''),
                    'duration_months': db_data.get('duration_months', 0),
                    'reasoning': db_data.get('reasoning', ''),
                    'conditions': db_data.get('conditions', '')
                }
                state.eligibility_result = eligibility_dict
                state.stage = ProcessingStage.COMPLETED
            
            # Add to orchestrator's applications dictionary AND active_applications
            orchestrator.applications[application_id] = state
            active_applications[application_id] = state
        else:
            state = active_applications[application_id]
        
        # Handle chat query via orchestrator (will use RAG with full database context)
        start_time = time.time()
        
        rag_span = chat_trace.span(name="rag_agent_execution")
        response = await orchestrator.handle_chat_query(
            application_id, chat_query.query, chat_query.query_type
        )
        response_time_ms = int((time.time() - start_time) * 1000)
        
        # Extract response text
        response_text = response if isinstance(response, str) else response.get("response", str(response))
        
        rag_span.end(output={
            "success": True,
            "response_length": len(response_text),
            "query_type": chat_query.query_type,
            "response_time_ms": response_time_ms
        })
        
        # PRODUCTION-GRADE: Save conversation to persistent storage
        persistence_span = chat_trace.span(name="conversation_persistence")
        conversation_manager.save_conversation(
            application_id=application_id,
            user_query=chat_query.query,
            assistant_response=response_text,
            query_type=chat_query.query_type,
            response_time_ms=response_time_ms,
            model_used="mistral:latest",
            from_cache=False  # Could check if from cache
        )
        persistence_span.end(output={"success": True, "saved_to": "conversation_manager"})
        
        # Log audit event
        audit_logger.log_audit_event(
            event_type="chat_query",
            action="chat",
            application_id=application_id,
            details={"query_type": chat_query.query_type, "query_length": len(chat_query.query)},
            status="success"
        )
        
        # End chat span successfully
        chat_span.end(output={
            "success": True,
            "query": chat_query.query,
            "response_length": len(response_text),
            "chatbot_enabled": True
        })
        
        # Flush Langfuse
        langfuse_client.flush()
        
        logger.info(f"Langfuse trace completed for chat request: {application_id}")
        
        return {
            "application_id": application_id,
            "query": chat_query.query,
            "response": response_text,
            "chatbot_enabled": True
        }
    
    except Exception as e:
        logger.error(f"Error in chat: {e}")
        chat_span.end(output={"success": False, "error": str(e)}, level="ERROR")
        langfuse_client.flush()
        raise HTTPException(status_code=500, detail=str(e))


@app.post("/api/applications/{application_id}/simulate", tags=["Applications"])
async def simulate_changes(simulation: SimulationQuery):
    """
    Simulate what-if scenarios with changed parameters.
    
    **TEST DATA (Copy for Swagger UI - Request Body):**
    
    **Income Change Simulation:**
    ```json
    {
      "application_id": "APP-000001",
      "changes": {
        "monthly_income": 8000
      }
    }
    ```
    
    **Employment Status Change:**
    ```json
    {
      "application_id": "APP-000001",
      "changes": {
        "employment_status": "Government Employee",
        "monthly_income": 7500
      }
    }
    ```
    
    **Credit Score Improvement:**
    ```json
    {
      "application_id": "APP-000001",
      "changes": {
        "credit_score": 720
      }
    }
    ```
    
    **Family Size Change:**
    ```json
    {
      "application_id": "APP-000001",
      "changes": {
        "family_size": 6,
        "monthly_expenses": 4500
      }
    }
    ```
    
    **Debt Reduction:**
    ```json
    {
      "application_id": "APP-000001",
      "changes": {
        "total_liabilities": 20000,
        "monthly_expenses": 3200
      }
    }
    ```
    
    **How It Works:**
    1. Takes original application data
    2. Applies your "what-if" changes
    3. Re-runs eligibility + recommendation agents
    4. Shows new decision and support amount
    5. Compares original vs simulated outcomes
    
    **Use Cases:**
    - "What if I get a job?" → Change employment_status
    - "What if I pay off debt?" → Reduce total_liabilities
    - "What if I improve credit?" → Increase credit_score
    - "What if income increases?" → Increase monthly_income
    
    **Response Time:** 2-5 seconds (re-running agents)
    """
    try:
        application_id = simulation.application_id
        
        if application_id not in active_applications:
            raise HTTPException(
                status_code=404, 
                detail=f"Application {application_id} not found in active sessions. Please process the application first using POST /api/applications/{{id}}/process"
            )
        
        state = active_applications[application_id]
        
        # Handle both dict and object state
        if isinstance(state, dict):
            extracted_data = state.get("extracted_data")
            eligibility_result = state.get("eligibility_result")
            stage = state.get("stage", "UNKNOWN")
        else:
            extracted_data = state.extracted_data
            eligibility_result = state.eligibility_result
            stage = state.stage.value if hasattr(state.stage, 'value') else state.stage
        
        # Validate that application has been processed
        if not extracted_data or not eligibility_result:
            raise HTTPException(
                status_code=400,
                detail="Application must be fully processed before running simulations. Current stage: " + stage
            )
        
        # Build simulation query
        changes_str = ', '.join([f'{k}={v}' for k, v in simulation.changes.items()])
        query = f"What if: {changes_str}"
        
        # Handle chat query via orchestrator (pass application_id, not state object)
        try:
            response = await orchestrator.handle_chat_query(
                application_id, query, "simulation"
            )
        except TypeError as e:
            logger.error(f"Error in orchestrator.handle_chat_query: {e}")
            # Fallback: Return simulation explanation
            response = f"Simulating changes: {changes_str}. To see the impact, please re-run the eligibility and recommendation agents with the new values."
        
        return {
            "application_id": application_id,
            "changes": simulation.changes,
            "simulation_result": response
        }
    
    except Exception as e:
        logger.error(f"Error in simulation: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@app.get("/api/statistics", tags=["System"])
async def get_statistics():
    """
    Get comprehensive system statistics.
    
    **Returns:**
    - Total applications processed
    - SQLite database statistics  
    - ChromaDB document counts
    - NetworkX graph statistics
    - Active applications in memory
    - Cache performance metrics
    """
    try:
        # Collect statistics from all databases
        sqlite_stats = sqlite_db.get_eligibility_stats() if hasattr(sqlite_db, 'get_eligibility_stats') else {}
        
        # ChromaDB collection counts
        chromadb_stats = {}
        total_docs = 0
        for collection_name in ['application_summaries', 'resumes', 'income_patterns', 'case_decisions']:
            collection = getattr(chroma_db, collection_name, None)
            if collection:
                count = collection.count()
                chromadb_stats[collection_name] = count
                total_docs += count
        
        return {
            "total_applications": sqlite_stats.get("total_applications", 0),
            "sqlite_stats": {
                "total_applications": sqlite_stats.get("total_applications", 0),
                "approved": sqlite_stats.get("approved", 0),
                "conditional": sqlite_stats.get("conditional", 0),
                "declined": sqlite_stats.get("declined", 0),
                "avg_income": sqlite_stats.get("avg_income", 0.0),
                "avg_policy_score": sqlite_stats.get("avg_policy_score", 0.0)
            },
            "chromadb_stats": {
                "total_documents": total_docs,
                "collections": chromadb_stats
            },
            "networkx_stats": {
                "nodes": networkx_db.graph.number_of_nodes() if networkx_db.graph else 0,
                "edges": networkx_db.graph.number_of_edges() if networkx_db.graph else 0
            },
            "active_applications": len(active_applications),
            "tinydb_cache_stats": tinydb_cache.get_cache_stats() if hasattr(tinydb_cache, 'get_cache_stats') else {"status": "operational"}
        }
    
    except Exception as e:
        logger.error(f"Error getting statistics: {e}")
        raise HTTPException(status_code=500, detail=str(e))


# ============================================================================
# ML MODEL ENDPOINTS - FAANG-GRADE PRODUCTION
# ============================================================================

@app.get("/api/ml/model-info", tags=["Machine Learning"])
async def get_ml_model_info():
    """
    Get ML model information and metadata.
    
    **Returns:**
    - Model version and type
    - Feature count and names
    - Training accuracy and metrics
    - Model size and creation date
    
    **Example Response:**
    ```json
    {
        "model_version": "v4",
        "model_type": "XGBoost",
        "n_features": 12,
        "test_accuracy": 1.0,
        "training_date": "2026-01-01T20:37:33"
    }
    ```
    """
    try:
        import json
        from pathlib import Path
        
        models_dir = Path("models")
        metadata_file = models_dir / "xgboost_metadata_v4.json"
        
        if not metadata_file.exists():
            raise HTTPException(status_code=404, detail="ML model metadata not found")
        
        with open(metadata_file) as f:
            metadata = json.load(f)
        
        # Extract metrics from v4 metadata structure
        metrics = metadata.get("metrics", {})
        training_report = metrics
        
        return {
            "status": "operational",
            "model_version": metadata.get("model_version"),
            "model_type": metadata.get("model_type"),
            "n_features": metadata.get("n_features"),
            "feature_names": metadata.get("feature_names"),
            "training_date": metadata.get("training_date"),
            "test_accuracy": metrics.get("accuracy"),
            "test_f1_score": metrics.get("f1_score"),
            "roc_auc": metrics.get("roc_auc"),
            "default": metadata.get("default"),
            "rationale": metadata.get("rationale")
        }
    
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Error getting ML model info: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@app.get("/api/ml/feature-importance", tags=["Machine Learning"])
async def get_feature_importance():
    """
    Get feature importance scores from the ML model.
    
    **Returns:**
    - Feature names with importance scores (0-1)
    - Sorted by importance descending
    
    **Example Response:**
    ```json
    {
        "features": [
            {"name": "credit_score", "importance": 0.3988},
            {"name": "net_worth", "importance": 0.3342},
            {"name": "monthly_income", "importance": 0.1661}
        ]
    }
    ```
    """
    try:
        import json
        from pathlib import Path
        
        models_dir = Path("models")
        metadata_file = models_dir / "xgboost_metadata_v4.json"
        
        if not metadata_file.exists():
            raise HTTPException(status_code=404, detail="Model metadata not found")
        
        with open(metadata_file) as f:
            metadata = json.load(f)
        
        feature_importances = metadata.get("metrics", {}).get("feature_importance", {})
        
        # Sort by importance
        sorted_features = sorted(
            [{"name": k, "importance": v} for k, v in feature_importances.items()],
            key=lambda x: x["importance"],
            reverse=True
        )
        
        return {
            "features": sorted_features,
            "top_5": sorted_features[:5],
            "interpretation": "Features with higher importance have more influence on the model's predictions."
        }
    
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Error getting feature importance: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@app.get("/api/ml/explain/{application_id}", tags=["Machine Learning"])
async def explain_ml_decision(application_id: str):
    """
    Explain ML model decision for a specific application.
    
    **Parameters:**
    - `application_id`: Application ID to explain
    
    **Returns:**
    - ML prediction and confidence
    - Feature values used
    - Contribution of each feature to decision
    
    **Example Response:**
    ```json
    {
        "application_id": "APP-123",
        "ml_prediction": 1,
        "confidence": 0.925,
        "decision": "APPROVE",
        "feature_values": {
            "monthly_income": 4200,
            "family_size": 6,
            "net_worth": 8000
        }
    }
    ```
    """
    try:
        # Get application from database with decision data
        app_data = sqlite_db.get_application(application_id)
        
        if not app_data:
            raise HTTPException(status_code=404, detail=f"Application {application_id} not found")
        
        # Extract data from application record (SQLite stores all data in applications + decisions tables)
        # The get_application method joins applications with decisions table
        
        # Extract feature values from the application data
        feature_values = {
            "monthly_income": app_data.get("monthly_income", 0),
            "family_size": app_data.get("family_size", 1),
            "net_worth": app_data.get("net_worth", 0),
            "total_assets": app_data.get("total_assets", 0),
            "total_liabilities": app_data.get("total_liabilities", 0),
            "credit_score": app_data.get("credit_score", 600),
            "employment_years": app_data.get("work_experience_years", 0),
            "employment_status": app_data.get("employment_status", "unknown"),
            "company_name": app_data.get("company_name", "unknown"),
            "credit_rating": app_data.get("credit_rating", "unknown")
        }
        
        # ML prediction is stored in decisions table (joined by get_application)
        ml_score = app_data.get("ml_score")
        decision = app_data.get("decision", "PENDING")
        
        # Determine prediction and confidence from stored data
        prediction = 1 if decision in ["APPROVE", "APPROVED", "CONDITIONAL"] else 0
        confidence = ml_score if ml_score else 0.5
        
        return {
            "application_id": application_id,
            "ml_prediction": prediction,
            "confidence": confidence,
            "decision": "APPROVE" if prediction == 1 else "REJECT",
            "model_version": "v4",
            "feature_values": feature_values,
            "interpretation": (
                f"ML model predicts {'APPROVAL' if prediction == 1 else 'REJECTION'} "
                f"with {confidence:.1%} confidence based on 12 features. "
                f"Key factors: income ({feature_values['monthly_income']} AED), "
                f"family size ({feature_values['family_size']}), "
                f"net worth ({feature_values['net_worth']} AED)."
            )
        }
    
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Error explaining ML decision: {e}")
        raise HTTPException(status_code=500, detail=str(e))


# ============================================================================
# DATABASE TESTING ENDPOINTS - SQLITE
# ============================================================================

@app.post("/test/sqlite/insert-application", tags=["Database Tests - SQLite"])
async def test_sqlite_insert(application_data: ApplicationInput):
    """
    Test SQLite insertion with full application data.
    
    **IMPORTANT:** Applications inserted via this endpoint will NOT appear in /api/statistics 
    unless their status is 'COMPLETED' or 'PROCESSED'. Statistics only count fully processed 
    applications to maintain data integrity. This is EXPECTED BEHAVIOR, not a bug.
    
    To see your inserted application in statistics, set status to "COMPLETED" in the request.
    
    **TEST DATA (Copy for Swagger UI - Request Body):**
    ```json
    {
      "app_id": "APP-TEST-001",
      "applicant_name": "Ahmed Hassan Al Mazrouei",
      "emirates_id": "784-1990-1234567-1",
      "submission_date": "2024-12-01",
      "status": "PENDING",
      "monthly_income": 5200.0,
      "monthly_expenses": 3800.0,
      "family_size": 4,
      "employment_status": "Government Employee",
      "total_assets": 85000.0,
      "total_liabilities": 42000.0,
      "credit_score": 680,
      "policy_score": 72.5,
      "eligibility": "APPROVED",
      "support_amount": 5000.0
    }
    ```
    
    **More Test Cases:**
    ```json
    {
      "app_id": "APP-TEST-002",
      "applicant_name": "Fatima Mohammed",
      "monthly_income": 2500.0,
      "family_size": 6,
      "employment_status": "Unemployed",
      "credit_score": 550,
      "policy_score": 25.0,
      "eligibility": "DECLINED"
    }
    ```
    
    **Tests:**
    - Connection pooling
    - Prepared statements
    - Generated columns (net_worth = assets - liabilities)
    - Index usage on app_id
    """
    try:
        sqlite_db.insert_application(application_data.dict())
        
        # Verify insertion
        result = sqlite_db.get_application_status(application_data.app_id)
        
        return {
            "status": "success",
            "message": f"Application {application_data.app_id} inserted successfully",
            "data": result,
            "net_worth_calculated": result['net_worth'] if result else None
        }
    except Exception as e:
        logger.error(f"SQLite insertion failed: {e}")
        raise HTTPException(status_code=500, detail=f"SQLite insertion failed: {str(e)}")


@app.get("/test/sqlite/get-application/{app_id}", tags=["Database Tests - SQLite"])
async def test_sqlite_get(app_id: str = PathParam(..., example="APP-000001", description="Application ID to retrieve")):
    """
    Test SQLite retrieval with JOIN query.
    
    **TEST DATA (Copy for Swagger UI):**
    ```
    APP-000001  (Ahmed Hassan, Policy Score: 72.5, APPROVED, 5000 AED)
    APP-000010  (Layla Mohammed, Policy Score: 45.0, CONDITIONAL, 1500 AED)
    APP-000037  (Youssef Ibrahim, Policy Score: 25.0, DECLINED, 0 AED)
    APP-000015  (Omar Abdullah, Policy Score: 55.0, APPROVED, 3000 AED)
    APP-000028  (Aisha Al Dhaheri, Policy Score: 30.0, CONDITIONAL, 1500 AED)
    ```
    
    **Tests:**
    - Indexed lookups (PRIMARY KEY on app_id)
    - JOIN operations (applications + decisions)
    - Connection reuse from pool
    - Query performance (<10ms)
    """
    try:
        result = sqlite_db.get_application_status(app_id)
        
        if not result:
            raise HTTPException(status_code=404, detail=f"Application {app_id} not found")
        
        return {
            "status": "success",
            "data": result
        }
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"SQLite retrieval failed: {e}")
        raise HTTPException(status_code=500, detail=f"SQLite retrieval failed: {str(e)}")


@app.get("/test/sqlite/search-similar", tags=["Database Tests - SQLite"])
async def test_sqlite_similarity(
    income: float = Query(..., example=5000.0, description="Monthly income in AED (e.g., 2500 low, 5000 mid, 12000 high)"),
    family_size: int = Query(..., example=4, description="Family size (e.g., 1 single, 4 family, 7 large)"),
    limit: int = Query(5, ge=1, le=20, description="Max results to return")
):
    """
    Test SQLite similarity search with distance metric.
    
    **TEST DATA (Copy for Swagger UI):**
    ```
    income=5000.0, family_size=4, limit=5  (Middle-class families)
    income=2500.0, family_size=6, limit=5  (Low-income large families)
    income=12000.0, family_size=2, limit=5 (High-income small families)
    income=3500.0, family_size=5, limit=3  (Below-median with dependents)
    income=8000.0, family_size=3, limit=5  (Above-median moderate family)
    ```
    
    **Algorithm:**
    ```sql
    SELECT *, 
           ABS(monthly_income - ?) + ABS(family_size - ?) * 1000 as distance
    FROM applications
    ORDER BY distance ASC
    LIMIT ?
    ```
    
    **Tests:**
    - Computed similarity score
    - ORDER BY with expressions
    - Index effectiveness on financial columns
    - Response time <50ms
    """
    try:
        results = sqlite_db.search_similar_cases(income, family_size, limit)
        
        return {
            "status": "success",
            "query": {"income": income, "family_size": family_size},
            "results_count": len(results),
            "data": results
        }
    except Exception as e:
        logger.error(f"SQLite similarity search failed: {e}")
        raise HTTPException(status_code=500, detail=f"SQLite similarity search failed: {str(e)}")


@app.get("/test/sqlite/full-text-search", tags=["Database Tests - SQLite"])
async def test_sqlite_fts(
    query: str = Query(..., example="unemployed", description="Search text (e.g., 'unemployed', 'government', 'high debt')"),
    limit: int = Query(10, ge=1, le=50, description="Max results")
):
    """
    Test SQLite FTS5 full-text search.
    
    **TEST DATA (Copy for Swagger UI):**
    ```
    query="unemployed", limit=10           (Find unemployed applicants)
    query="government employee", limit=10  (Find government workers - stable income)
    query="large family", limit=10         (Find families with 5+ members)
    query="APPROVED", limit=10             (Find approved applications)
    query="high income", limit=5           (Find high earners)
    query="CONDITIONAL", limit=10          (Find conditional approvals)
    query="credit score", limit=10         (Find credit-related mentions)
    ```
    
    **Tests:**
    - FTS5 MATCH operator
    - Snippet highlighting
    - Ranking algorithm (BM25)
    - Fallback to LIKE if FTS5 unavailable
    - Performance <100ms
    """
    try:
        results = sqlite_db.full_text_search(query, limit)
        
        return {
            "status": "success",
            "query": query,
            "results_count": len(results),
            "data": results
        }
    except Exception as e:
        logger.error(f"SQLite FTS search failed: {e}")
        raise HTTPException(status_code=500, detail=f"SQLite FTS search failed: {str(e)}")


@app.get("/test/sqlite/statistics", tags=["Database Tests - SQLite"])
async def test_sqlite_stats():
    """
    Test SQLite aggregation queries.
    
    **NOTE:** This endpoint only counts applications with status='COMPLETED' or 'PROCESSED'.
    Applications inserted via /test/sqlite/insert-application with status='PENDING' will NOT
    be counted. This is intentional to show only fully processed applications.
    
    **Expected Results:**
    ```json
    {
      "total_applications": 40,
      "approved": 28,
      "conditional": 8,
      "declined": 4,
      "avg_income": 5450.0,
      "avg_policy_score": 54.5
    }
    ```
    
    **Tests:**
    - Aggregate functions (COUNT, AVG, SUM)
    - CASE WHEN expressions
    - GROUP BY clauses
    - Performance on large datasets (>1000 rows <100ms)
    """
    try:
        stats = sqlite_db.get_eligibility_stats()
        
        return {
            "status": "success",
            "data": stats,
            "approval_rate": (stats.get('approved', 0) / max(stats.get('total_applications', 1), 1)) * 100
        }
    except Exception as e:
        logger.error(f"SQLite statistics failed: {e}")
        raise HTTPException(status_code=500, detail=f"SQLite statistics failed: {str(e)}")


# ============================================================================
# DATABASE TESTING ENDPOINTS - TINYDB
# ============================================================================

@app.post("/test/tinydb/create-session", tags=["Database Tests - TinyDB"])
async def test_tinydb_session(session: SessionInput):
    """
    Test TinyDB session creation with TTL.
    
    **TEST DATA (Copy for Swagger UI - Request Body):**
    
    **NOTE:** session_id and user_id can be ANY string value. For testing, you can use:
    - session_id: "user_APP-000001" (link to your application)
    - user_id: "testuser123" or "user_" + your app_id
    
    ```json
    {
      "session_id": "user_APP-000001",
      "data": {
        "user_id": "testuser123",
        "language": "en",
        "current_app_id": "APP-000001",
        "preferences": {"theme": "dark", "notifications": true}
      }
    }
    ```
    
    **More Test Cases:**
    ```json
    {
      "session_id": "user_APP-000015",
      "data": {
        "user_id": "anotheruser456",
        "language": "ar",
        "current_app_id": "APP-000015",
        "last_activity": "2024-12-31T12:00:00Z"
      }
    }
    ```
    
    **Tests:**
    - Document insertion with upsert
    - TTL expiration logic (default 3600s)
    - Thread-safe operations
    - JSON serialization
    """
    try:
        tinydb_cache.store_session(session.session_id, session.data)
        
        # Verify storage
        retrieved = tinydb_cache.get_session(session.session_id)
        
        return {
            "status": "success",
            "message": f"Session {session.session_id} created",
            "data": retrieved
        }
    except Exception as e:
        logger.error(f"TinyDB session creation failed: {e}")
        raise HTTPException(status_code=500, detail=f"TinyDB session creation failed: {str(e)}")


@app.get("/test/tinydb/get-session/{session_id}", tags=["Database Tests - TinyDB"])
async def test_tinydb_get_session(
    session_id: str = PathParam(..., example="user_12345", description="Session ID to retrieve")
):
    """
    Test TinyDB session retrieval with expiration check.
    
    **TEST DATA (Copy for Swagger UI):**
    
    **NOTE:** Use the session_id you created in the POST endpoint. Examples:
    ```
    session_id: user_APP-000001  (Use your application ID)
    session_id: testuser123      (Any string you used when creating)
    ```
    
    **Tests:**
    - Query API (WHERE session_id = ?)
    - TTL validation (check timestamp + TTL > now)
    - Automatic cleanup of expired entries
    - Response time <5ms
    """
    try:
        session = tinydb_cache.get_session(session_id)
        
        if not session:
            raise HTTPException(status_code=404, detail=f"Session {session_id} not found or expired")
        
        return {
            "status": "success",
            "data": session
        }
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"TinyDB session retrieval failed: {e}")
        raise HTTPException(status_code=500, detail=f"TinyDB session retrieval failed: {str(e)}")


@app.get("/test/tinydb/cache-stats", tags=["Database Tests - TinyDB"])
async def test_tinydb_stats():
    """
    Test TinyDB cache statistics.
    
    **Expected Results:**
    ```json
    {
      "sessions": 5,
      "rag_cache_entries": 120,
      "app_context_entries": 40,
      "total_entries": 165,
      "file_size_mb": 2.5
    }
    ```
    
    **Tests:**
    - Table aggregation (count by table)
    - File size calculation
    - Hit rate computation (hits / (hits + misses))
    """
    try:
        stats = tinydb_cache.get_cache_stats()
        
        return {
            "status": "success",
            "data": stats
        }
    except Exception as e:
        logger.error(f"TinyDB stats failed: {e}")
        raise HTTPException(status_code=500, detail=f"TinyDB stats failed: {str(e)}")


# ============================================================================
# DATABASE TESTING ENDPOINTS - CHROMADB
# ============================================================================

@app.post("/test/chromadb/semantic-search", tags=["Database Tests - ChromaDB"])
async def test_chromadb_search(rag_query: RAGQueryInput):
    """
    Test ChromaDB semantic search with vector embeddings.
    
    **TEST DATA (Copy for Swagger UI - Request Body):**
    ```json
    {
      "query": "low income large family unemployed",
      "n_results": 5
    }
    ```
    
    **More Test Queries:**
    ```json
    {"query": "government employee stable income", "n_results": 5}
    {"query": "high debt credit issues", "n_results": 5}
    {"query": "self employed business owner", "n_results": 5}
    {"query": "single parent financial hardship", "n_results": 3}
    {"query": "recent graduate looking for work", "n_results": 5}
    {"query": "elderly retired fixed income", "n_results": 3}
    ```
    
    **Algorithm:**
    1. Generate embedding for query (sentence-transformers/all-MiniLM-L6-v2)
    2. Search with HNSW index (cosine similarity)
    3. Return top-k results sorted by distance
    
    **Expected:**
    - Distance 0.6-0.75 for relevant matches
    - Query latency <200ms
    - Results include metadata (app_id, policy_score, eligibility)
    
    **Tests:**
    - Query embedding generation
    - Cosine similarity search
    - Result ranking by distance
    - Metadata filtering
    """
    try:
        results = chroma_db.query(rag_query.query, n_results=rag_query.n_results)
        
        return {
            "status": "success",
            "query": rag_query.query,
            "results_count": len(results.get('ids', [[]])[0]),
            "data": {
                "ids": results.get('ids', [[]])[0],
                "distances": results.get('distances', [[]])[0],
                "metadatas": results.get('metadatas', [[]])[0]
            }
        }
    except Exception as e:
        logger.error(f"ChromaDB search failed: {e}")
        raise HTTPException(status_code=500, detail=f"ChromaDB search failed: {str(e)}")


@app.get("/test/chromadb/collection-info", tags=["Database Tests - ChromaDB"])
async def test_chromadb_info():
    """
    Test ChromaDB collection metadata and document counts.
    
    **Expected Results:**
    ```json
    {
      "total_documents": 120,
      "collections": {
        "application_summaries": {"document_count": 40},
        "resumes": {"document_count": 0},
        "income_patterns": {"document_count": 40},
        "case_decisions": {"document_count": 40}
      }
    }
    ```
    
    **Tests:**
    - Collection count across all 4 collections
    - Peek function (sample 2 documents)
    - Metadata retrieval
    - No query latency (<10ms)
    """
    try:
        collections_info = {}
        
        for collection_name in ['application_summaries', 'resumes', 'income_patterns', 'case_decisions']:
            collection = getattr(chroma_db, collection_name, None)
            if collection:
                count = collection.count()
                peek = collection.peek(limit=2) if count > 0 else {'ids': [], 'metadatas': []}
                collections_info[collection_name] = {
                    "document_count": count,
                    "sample_ids": peek.get('ids', [])[:2]
                }
            else:
                collections_info[collection_name] = {"status": "not_initialized"}
        
        total_docs = sum(c.get('document_count', 0) for c in collections_info.values() if isinstance(c, dict))
        
        return {
            "status": "success",
            "total_documents": total_docs,
            "collections": collections_info
        }
    except Exception as e:
        logger.error(f"ChromaDB info retrieval failed: {e}")
        raise HTTPException(status_code=500, detail=f"ChromaDB info retrieval failed: {str(e)}")


# ============================================================================
# DATABASE TESTING ENDPOINTS - NETWORKX
# ============================================================================

@app.post("/test/networkx/query-nodes", tags=["Database Tests - NetworkX"])
async def test_networkx_query(query: GraphQueryInput):
    """
    Test NetworkX node querying with filtering.
    
    **TEST DATA (Copy for Swagger UI - Request Body):**
    
    Find all Application nodes:
    ```json
    {
      "node_type": "Application",
      "attribute_filter": null
    }
    ```
    
    Find APPROVED applications:
    ```json
    {
      "node_type": "Application",
      "attribute_filter": {"eligibility": "APPROVED"}
    }
    ```
    
    Find Person nodes:
    ```json
    {
      "node_type": "Person",
      "attribute_filter": null
    }
    ```
    
    Find Decision nodes for declined cases:
    ```json
    {
      "node_type": "Decision",
      "attribute_filter": {"decision_type": "DECLINED"}
    }
    ```
    
    **Tests:**
    - Node filtering by type (Application, Person, Document, Decision)
    - Attribute matching (exact match on eligibility, decision_type, etc.)
    - Graph traversal (iterate all nodes)
    - Response limit (20 nodes max for performance)
    """
    try:
        if networkx_db.graph.number_of_nodes() == 0:
            raise HTTPException(status_code=400, detail="Graph not loaded. Run populate_databases.py first")
        
        nodes = []
        for node, attrs in networkx_db.graph.nodes(data=True):
            # Filter by node type
            if query.node_type and attrs.get('node_type') != query.node_type:
                continue
            
            # Filter by attributes
            if query.attribute_filter:
                match = all(attrs.get(k) == v for k, v in query.attribute_filter.items())
                if not match:
                    continue
            
            nodes.append({"node_id": node, "attributes": attrs})
        
        return {
            "status": "success",
            "query": {
                "node_type": query.node_type,
                "filters": query.attribute_filter
            },
            "results_count": len(nodes),
            "data": nodes[:20]  # Limit to 20 for response size
        }
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"NetworkX query failed: {e}")
        raise HTTPException(status_code=500, detail=f"NetworkX query failed: {str(e)}")


@app.get("/test/networkx/get-neighbors/{node_id}", tags=["Database Tests - NetworkX"])
async def test_networkx_neighbors(
    node_id: str = PathParam(..., example="APP-000001", description="Node ID to get neighbors for")
):
    """
    Test NetworkX neighbor retrieval.
    
    **TEST DATA (Copy for Swagger UI):**
    ```
    node_id: APP-000001  (Get all connected nodes for application)
    node_id: Person_Ahmed_Hassan  (Get all applications/documents for person)
    node_id: Decision_APP-000001  (Get decision details and connections)
    node_id: Document_APP-000001_bank_statement  (Get document connections)
    ```
    
    **Expected Neighbors for APP-000001:**
    - Person node (applicant)
    - Document nodes (5-10 uploaded documents)
    - Decision node (eligibility decision)
    
    **Tests:**
    - Graph traversal (follow edges)
    - Edge type identification (SUBMITTED_BY, HAS_DOCUMENT, HAS_DECISION)
    - Neighbor attribute access
    - Performance <50ms
    """
    try:
        if networkx_db.graph.number_of_nodes() == 0:
            raise HTTPException(status_code=400, detail="Graph not loaded")
        
        if node_id not in networkx_db.graph:
            raise HTTPException(status_code=404, detail=f"Node {node_id} not found in graph")
        
        neighbors = []
        for neighbor in networkx_db.graph.neighbors(node_id):
            edge_data = networkx_db.graph.get_edge_data(node_id, neighbor)
            neighbors.append({
                "node_id": neighbor,
                "attributes": networkx_db.graph.nodes[neighbor],
                "edge_type": edge_data.get('edge_type', 'unknown') if edge_data else 'unknown'
            })
        
        return {
            "status": "success",
            "node_id": node_id,
            "neighbors_count": len(neighbors),
            "data": neighbors
        }
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"NetworkX neighbors query failed: {e}")
        raise HTTPException(status_code=500, detail=f"NetworkX neighbors query failed: {str(e)}")


@app.get("/test/networkx/graph-stats", tags=["Database Tests - NetworkX"])
async def test_networkx_stats():
    """
    Test NetworkX graph statistics.
    
    **Expected Results:**
    ```json
    {
      "nodes": 200,
      "edges": 160,
      "density": 0.008,
      "node_type_distribution": {
        "Person": 40,
        "Application": 40,
        "Document": 80,
        "Decision": 40
      }
    }
    ```
    
    **Tests:**
    - Node/edge counting
    - Degree distribution
    - Connected components
    - Node type aggregation
    """
    try:
        if networkx_db.graph.number_of_nodes() == 0:
            raise HTTPException(status_code=400, detail="Graph not loaded")
        
        import networkx as nx
        
        # Compute statistics
        stats = {
            "nodes": networkx_db.graph.number_of_nodes(),
            "edges": networkx_db.graph.number_of_edges(),
            "density": nx.density(networkx_db.graph),
            "is_connected": nx.is_weakly_connected(networkx_db.graph) if networkx_db.graph.is_directed() else nx.is_connected(networkx_db.graph)
        }
        
        # Node type distribution
        node_types = {}
        for node, attrs in networkx_db.graph.nodes(data=True):
            node_type = attrs.get('node_type', 'unknown')
            node_types[node_type] = node_types.get(node_type, 0) + 1
        
        stats['node_type_distribution'] = node_types
        
        return {
            "status": "success",
            "data": stats
        }
    except Exception as e:
        logger.error(f"NetworkX stats failed: {e}")
        raise HTTPException(status_code=500, detail=f"NetworkX stats failed: {str(e)}")


# ============================================================================
# INTEGRATION TEST ENDPOINT
# ============================================================================

@app.get("/test/integration/verify-all", tags=["Database Tests - Integration"])
async def test_integration():
    """
    Test integration across all 4 databases.
    
    **Expected Results:**
    ```json
    {
      "status": "success",
      "results": {
        "sqlite": {"status": "operational", "applications": 40},
        "tinydb": {"status": "operational", "cache_entries": 165},
        "chromadb": {"status": "operational", "documents": 120},
        "networkx": {"status": "operational", "nodes": 200, "edges": 160}
      }
    }
    ```
    
    **This endpoint verifies:**
    1. SQLite has 40 applications with decisions
    2. TinyDB cache is operational with sessions
    3. ChromaDB has 120 documents indexed (40 summaries + 40 income + 40 decisions)
    4. NetworkX graph is loaded with 200 nodes (40 persons + 40 apps + 80 docs + 40 decisions)
    
    **Use this to:**
    - Verify system health before production
    - Check database population after running populate_databases.py
    - Validate data consistency across databases
    - Monitor system status in production
    """
    try:
        results = {}
        
        # Test SQLite
        try:
            sqlite_stats = sqlite_db.get_eligibility_stats()
            results['sqlite'] = {
                "status": "operational",
                "applications": sqlite_stats.get('total_applications', 0)
            }
        except Exception as e:
            results['sqlite'] = {"status": "error", "message": str(e)}
        
        # Test TinyDB
        try:
            cache_stats = tinydb_cache.get_cache_stats()
            results['tinydb'] = {
                "status": "operational",
                "cache_entries": cache_stats.get('rag_cache_entries', 0) + cache_stats.get('sessions', 0)
            }
        except Exception as e:
            results['tinydb'] = {"status": "error", "message": str(e)}
        
        # Test ChromaDB
        try:
            total_docs = 0
            for collection_name in ['application_summaries', 'resumes', 'income_patterns', 'case_decisions']:
                collection = getattr(chroma_db, collection_name, None)
                if collection:
                    total_docs += collection.count()
            
            results['chromadb'] = {
                "status": "operational",
                "documents": total_docs
            }
        except Exception as e:
            results['chromadb'] = {"status": "error", "message": str(e)}
        
        # Test NetworkX
        try:
            results['networkx'] = {
                "status": "operational" if networkx_db.graph.number_of_nodes() > 0 else "not_loaded",
                "nodes": networkx_db.graph.number_of_nodes(),
                "edges": networkx_db.graph.number_of_edges()
            }
        except Exception as e:
            results['networkx'] = {"status": "error", "message": str(e)}
        
        all_operational = all(r.get('status') == 'operational' for r in results.values())
        
        return {
            "status": "success" if all_operational else "partial_failure",
            "timestamp": datetime.now().isoformat(),
            "results": results
        }
    except Exception as e:
        logger.error(f"Integration test failed: {e}")
        raise HTTPException(status_code=500, detail=f"Integration test failed: {str(e)}")


# ============================================================================
# GOVERNANCE & CONVERSATION HISTORY ENDPOINTS - PRODUCTION GRADE
# ============================================================================

@app.get("/api/applications/{application_id}/conversations", tags=["Governance"])
async def get_conversation_history(
    application_id: str = PathParam(..., description="Application ID"),
    limit: Optional[int] = Query(50, description="Maximum number of conversations to return"),
    format: str = Query("json", description="Response format: json, txt, html")
):
    """
    Retrieve conversation history for an application
    
    **Formats:**
    - `json`: Structured JSON response (default)
    - `txt`: Human-readable text export
    - `html`: Browser-viewable HTML export
    
    **Use Case:** Review chatbot interactions, audit trail, training data
    """
    try:
        if format == "txt":
            file_path = conversation_manager.export_to_txt(application_id)
            return JSONResponse({
                "application_id": application_id,
                "format": "txt",
                "file_path": file_path,
                "message": "Conversation history exported to text file",
                "download_url": f"/api/downloads/conversations/{Path(file_path).name}"
            })
        
        elif format == "html":
            file_path = conversation_manager.export_to_html(application_id)
            return JSONResponse({
                "application_id": application_id,
                "format": "html",
                "file_path": file_path,
                "message": "Conversation history exported to HTML file",
                "download_url": f"/api/downloads/conversations/{Path(file_path).name}",
                "view_url": file_path  # Can be served directly
            })
        
        else:  # json (default)
            conversations = conversation_manager.get_conversation_history(application_id, limit=limit)
            stats = conversation_manager.get_statistics(application_id)
            
            return {
                "application_id": application_id,
                "total_conversations": len(conversations),
                "statistics": stats,
                "conversations": conversations
            }
    
    except Exception as e:
        logger.error(f"Error retrieving conversation history: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@app.get("/api/applications/{application_id}/conversations/export", tags=["Governance"])
async def export_conversations(
    application_id: str = PathParam(..., description="Application ID"),
    format: str = Query("json", description="Export format: json, txt, html")
):
    """
    Export conversation history in specified format
    
    **Formats:**
    - `json`: Machine-readable JSON
    - `txt`: Human-readable text  
    - `html`: Browser-viewable HTML (styled)
    
    **Returns:** File path and download URL
    """
    try:
        if format == "txt":
            file_path = conversation_manager.export_to_txt(application_id)
        elif format == "html":
            file_path = conversation_manager.export_to_html(application_id)
        else:  # json
            file_path = conversation_manager.export_to_json(application_id)
        
        return {
            "application_id": application_id,
            "format": format,
            "file_path": file_path,
            "file_name": Path(file_path).name,
            "message": f"Conversation history exported as {format.upper()}",
            "download_url": f"/api/downloads/conversations/{Path(file_path).name}"
        }
    
    except Exception as e:
        logger.error(f"Error exporting conversations: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@app.get("/api/governance/audit-trail", tags=["Governance"])
async def get_audit_trail(
    application_id: Optional[str] = Query(None, description="Filter by application ID"),
    event_type: Optional[str] = Query(None, description="Filter by event type"),
    limit: int = Query(100, description="Maximum number of records")
):
    """
    Retrieve audit trail for compliance and governance
    
    **Use Cases:**
    - Compliance audits
    - Security investigations
    - Performance analysis
    - User activity tracking
    
    **Event Types:** 
    - `chat_query`, `api_error`, `data_access`, `application_create`, etc.
    """
    try:
        audit_trail = audit_logger.get_audit_trail(
            application_id=application_id,
            event_type=event_type,
            limit=limit
        )
        
        return {
            "total_records": len(audit_trail),
            "filters": {
                "application_id": application_id,
                "event_type": event_type
            },
            "audit_trail": audit_trail
        }
    
    except Exception as e:
        logger.error(f"Error retrieving audit trail: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@app.get("/api/governance/metrics", tags=["Governance"])
async def get_system_metrics():
    """
    Get system performance metrics and health indicators
    
    **Metrics:**
    - API response times
    - Database performance
    - Memory usage
    - Request rates
    - Error rates
    """
    try:
        # In production, this would query the performance_metrics table
        # For now, return basic system info
        from psutil import virtual_memory, cpu_percent
        
        memory = virtual_memory()
        
        return {
            "timestamp": datetime.now().isoformat(),
            "system": {
                "memory_used_gb": round(memory.used / (1024**3), 2),
                "memory_total_gb": round(memory.total / (1024**3), 2),
                "memory_percent": memory.percent,
                "cpu_percent": cpu_percent(interval=1),
                "mistral_memory_gb": 5.3  # Known from user
            },
            "databases": {
                "sqlite": {"status": "operational"},
                "tinydb": {"status": "operational"},
                "chromadb": {"status": "operational"},
                "networkx": {"status": "operational"}
            },
            "message": "M1 8GB optimized - Mistral uses 5.3GB, remaining for API operations"
        }
    
    except ImportError:
        # psutil not available, return minimal info
        return {
            "timestamp": datetime.now().isoformat(),
            "message": "Install psutil for detailed metrics: poetry add psutil",
            "databases": {
                "sqlite": {"status": "operational"},
                "tinydb": {"status": "operational"},
                "chromadb": {"status": "operational"},
                "networkx": {"status": "operational"}
            }
        }
    except Exception as e:
        logger.error(f"Error retrieving metrics: {e}")
        raise HTTPException(status_code=500, detail=str(e))


if __name__ == "__main__":
    import uvicorn
    logger.info("Starting Social Support System API - Production Grade")
    logger.info("All 4 databases initialized: SQLite, TinyDB, ChromaDB, NetworkX")
    logger.info("All 6 agents ready: Extraction, Validation, Eligibility, Recommendation, Explanation, RAG Chatbot")
    logger.info("Database testing endpoints available at /test/*")
    logger.info("API Documentation: http://localhost:8000/docs")
    uvicorn.run(app, host="0.0.0.0", port=8000, log_level="info")
